# -*- coding: utf-8 -*-
"""Suppy_Chain_Management_System_v3(collab).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pa8qPUtX7tU2TGiexhts0LBqKdXWTPYO

# Importing Libary and Dataset
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


df=pd.read_csv("D:\Project\Suppy chain management system\data\external\DataCoSupplyChainDataset.csv", encoding_errors="ignore")
pd.set_option('display.max_columns',None)

"""# Data Preprocessing"""

df.info()

df.head()

df.isna().sum()

df['Product Status'].value_counts(ascending = False)

"""**Null data**

Null data was found in three columns, Order Zipcode (86% null values, i.e. a total of 155679 records), Product description (100% null values, i.e. a total of 180519 records) and customer zipcode & customer Last Name (<0.1% is say a total of 3 records and 8 record respectively), together these empty cells correspond to 3.5% of the total dataset

**Categorical variables**

There are 28 variables that have been identified as categorical type, however, some variables were classified as categorical when they are numeric, for example Days for shipping and Order item quantity, this happens because all the values ​​are grouped into 4 discrete values, which which causes confusion on the part of pandas profiling. In order to carry out analysis, these variables must be considered as numerical.

**Numeric variables**

 There are 24 variables that have been identified as numeric type

**Customer Country**

Puerto Rico, an island in the Caribbean Sea, has been a territory of the United States since 1898, after the U.S. defeated Spain in the Spanish-American war. It’s classified as an “unincorporated territory,” meaning the island is controlled by the U.S. government but is separate from the mainland. So the customer country column will be of all United States. There is no use of the customer country column


**Conclusions**

We would not include in the analysis the columns order zipcode (almost empty), product description (totally empty), customer email (constant without information), Customer password (constant without information), customer zipcode column.

Finally, there are some columns that can be ignored when doing the analysis since they do not provide relevant information for the objective of the study, these columns are: Product Image, Customer Fname, Customer Lname, Product Description.
"""

df.drop(["Customer Email","Customer Password","Product Image","Customer Fname","Customer Lname","Product Description","Order Zipcode","Customer Zipcode","Customer Country","Order City","Customer Street","Order State","Product Status"],axis=1,inplace=True)

df.head()

df.rename(columns={"Type":"Type Of Payments"},inplace=True)

"""### Removing Redundancies

At this point, we can see that even the Order Customer Id and Customer Id are the same. It would make sense to make a function that check for all columns wether the values match to 100% to remove redundancies. Otherwiese one could opt to consult the makers of the dataset for further info
"""

from itertools import combinations

def check_redundancies(data):

    redundancy_list = []

    for i in list(combinations(data.columns, 2)):

        if all(data[i[0]] == data[i[1]]):
            redundancy_list.append(i)
            print("{} and {} are the same".format(*i))

    return redundancy_list

redundancies = check_redundancies(df)

df.drop(["Benefit per order","Order Item Total","Category Id","Customer Id","Order Item Cardprod Id","Order Item Product Price"],axis=1,inplace=True)

# Review columns:

df.info()

df['Customer State'].value_counts(ascending = False)

rows_to_delete = df[(df['Customer State'] == '95758') | (df['Customer State'] == '91732')].index

# Drop the identified rows
df = df.drop(rows_to_delete)

df['Category Name'].value_counts(ascending = False)

# We will create a function to insert all categories with count < 500 into 'Others'

categories = df['Category Name'].value_counts(ascending = False)
categories_under500 = categories[categories < 500]

def handle_categories(value):
    if(value in categories_under500):
        return 'Others'
    else:
        return value

df['Category Name'] = df['Category Name'].apply(handle_categories)

df['Category Name'].value_counts(ascending = False)

"""Format object column of "shipping date (DateOrders)" and "order date (DateOrders)" into datetime"""

df['shipping date (DateOrders)'] = pd.to_datetime(df['shipping date (DateOrders)'], format='%m/%d/%Y %H:%M').dt.strftime('%d/%m/%Y')

df['order date (DateOrders)'] = pd.to_datetime(df['order date (DateOrders)'], format='%m/%d/%Y %H:%M').dt.strftime('%d/%m/%Y')

df.head()

# Order Country

df['Order Country'].value_counts(ascending = False)

df['Order Country'].unique()

# As some countries in the list is in Spanaish, for futher use it is translated in English
corrections = {
    'Indonesia': 'Indonesia',
    'India': 'India',
    'Australia': 'Australia',
    'China': 'China',
    'Japn': 'Japan',
    'Corea del Sur': 'South Korea',
    'Singapur': 'Singapore',
    'Turqua': 'Turkey',
    'Butn': 'Bhutan',
    'Estados Unidos': 'United States',
    'Nigeria': 'Nigeria',
    'Repblica Democrtica del Congo': 'Democratic Republic of the Congo',
    'Marruecos': 'Morocco',
    'Alemania': 'Germany',
    'Francia': 'France',
    'Pases Bajos': 'Netherlands',
    'Reino Unido': 'United Kingdom',
    'Guatemala': 'Guatemala',
    'El Salvador': 'El Salvador',
    'Panam': 'Panama',
    'Repblica Dominicana': 'Dominican Republic',
    'Venezuela': 'Venezuela',
    'Colombia': 'Colombia',
    'Honduras': 'Honduras',
    'Brasil': 'Brazil',
    'Mxico': 'Mexico',
    'Argentina': 'Argentina',
    'Cuba': 'Cuba',
    'Per': 'Peru',
    'Nicaragua': 'Nicaragua',
    'Angola': 'Angola',
    'Egipto': 'Egypt',
    'Italia': 'Italy',
    'Espaa': 'Spain',
    'Suecia': 'Sweden',
    'Austria': 'Austria',
    'Canada': 'Canada',
    'Argelia': 'Algeria',
    'Sudfrica': 'South Africa',
    'Tanzania': 'Tanzania',
    'Nueva Zelanda': 'New Zealand',
    'Bangladesh': 'Bangladesh',
    'Tailandia': 'Thailand',
    'Irak': 'Iraq',
    'Arabia Saud': 'Saudi Arabia',
    'Filipinas': 'Philippines',
    'Irn': 'Iran',
    'Myanmar (Birmania)': 'Myanmar',
    'Ucrania': 'Ukraine',
    'Polonia': 'Poland',
    'Portugal': 'Portugal',
    'Rumania': 'Romania',
    'Pakistn': 'Pakistan',
    'Vietnam': 'Vietnam',
    'Malasia': 'Malaysia',
    'Rusia': 'Russia',
    'Irlanda': 'Ireland',
    'Noruega': 'Norway',
    'Blgica': 'Belgium',
    'Chile': 'Chile',
    'Suiza': 'Switzerland',
    'Hait': 'Haiti',
    'Tawin':'Taiwan',
     "Sudn": "Sudan",
    "Somalia": "Somalia",
    "Costa de Marfil": "Ivory Coast",
    "Egypt": "Egypt",
    "Italy": "Italy",
    "Spain": "Spain",
    "Sweden": "Sweden",
    "Nger": "Niger",
    "SudAfrica": "South Africa",
    "Mozambique": "Mozambique",
    "Tanzania": "Tanzania",
    "Ruanda": "Rwanda",
    "Israel": "Israel",
    "New Zealand": "New Zealand",
    "Banglads": "Bangladesh",
    "Philippines": "Philippines",
    "Kazajistn": "Kazakhstan",
    "Iran": "Iran",
    "Myanmar": "Myanmar",
    "Uzbekistn": "Uzbekistan",
    "Benn": "Benin",
    "Camern": "Cameroon",
    "Kenia": "Kenya",
    "Togo": "Togo",
    "Ukraine": "Ukraine",
    "Poland": "Poland",
    "Portugal": "Portugal",
    "Romania": "Romania",
    "Trinidad y Tobago": "Trinidad and Tobago",
    "Afganistn": "Afghanistan",
    "Pakistan": "Pakistan",
    "Vietnam": "Vietnam",
    "Malaysia": "Malaysia",
    "Finlandia": "Finland",
    "Russia": "Russia",
    "Ireland": "Ireland",
    "Norway": "Norway",
    "Eslovaquia": "Slovakia",
    "Belgium": "Belgium",
    "Bolivia": "Bolivia",
    "Chile": "Chile",
    "Jamaica": "Jamaica",
    "Yemen": "Yemen",
    "Ghana": "Ghana",
    "Guinea": "Guinea",
    "Etiopa": "Ethiopia",
    "Bulgaria": "Bulgaria",
    "Kirguistn": "Kyrgyzstan",
    "Georgia": "Georgia",
    "Nepal": "Nepal",
    "Emiratos rabes Unidos": "United Arab Emirates",
    "Camboya": "Cambodia",
    "Uganda": "Uganda",
    "Lesoto": "Lesotho",
    "Lituania": "Lithuania",
    "Switzerland": "Switzerland",
    "Hungra": "Hungary",
    "Dinamarca": "Denmark",
    "Haiti": "Haiti",
    "Bielorrusia": "Belarus",
    "Croacia": "Croatia",
    "Laos": "Laos",
    "Barin": "Barbados",
    "Macedonia": "North Macedonia",
    "Repblica Checa": "Czech Republic",
    "Sri Lanka": "Sri Lanka",
    "Zimbabue": "Zimbabwe",
    "Eritrea": "Eritrea",
    "Burkina Faso": "Burkina Faso",
    "Costa Rica": "Costa Rica",
    "Libia": "Libya",
    "Barbados": "Barbados",
    "Tayikistn": "Tajikistan",
    "Siria": "Syria",
    "Guadalupe": "Guadeloupe",
    "Papa Nueva Guinea": "Papua New Guinea",
    "Azerbaiyn": "Azerbaijan",
    "Turkmenistn": "Turkmenistan",
    "Paraguay": "Paraguay",
    "Jordania": "Jordan",
    "Hong Kong": "Hong Kong",
    "Martinica": "Martinique",
    "Moldavia": "Moldova",
    "Qatar": "Qatar",
    "Mali": "Mali",
    "Albania": "Albania",
    "Repblica del Congo": "Republic of the Congo",
    "Bosnia y Herzegovina": "Bosnia and Herzegovina",
    "Omn": "Oman",
    "Tnez": "Tunisia",
    "Sierra Leona": "Sierra Leone",
    "Yibuti": "Djibouti",
    "Burundi": "Burundi",
    "Montenegro": "Montenegro",
    "Gabn": "Gabon",
    "Shara Occidental":"Western Sahara",
    "Butn":"Bhutan",
    "Sudn del Sur":"South Sudan",
    'Luxemburgo':"Luxembourg",
    "Suazilandia":"Eswatini",
    "Guayana Francesa":"French Guiana",
    'Repblica Centroafricana':"Central African Republic",
    "Taiwn":"Taiwan",
    "Lbano":"Lebanon",
    'Eslovenia':"Slovenia",
    'Repblica de Gambia':'The Gambia',
    'Botsuana':"Botswana",
    'Guinea Ecuatorial':'Equatorial Guinea'
}

df['Order Country'] = df['Order Country'].replace(corrections)

df['Order Country'].unique()

# Sales

df['Sales'] = df['Sales'].round(2)

# Product Name

df['Product Name'].sample(10)

df['Product Name'].value_counts(ascending = False)

# We will group all product names < 1500 as 'Others'

products = df['Product Name'].value_counts(ascending = False)
products_under1500 = products[products < 1500]

def handle_products(value):
    if(value in products_under1500):
        return 'Others'
    else:
        return value

df['Product Name'] = df['Product Name'].apply(handle_products)

df['Product Name'].value_counts(ascending = False)

# Order Region

df['Order Region'].value_counts(ascending = False)

# Group multiple regions of North America into North America
# For object data, we will use 'index.isin()'

order_regions = df['Order Region'].value_counts(ascending = False)
n_america = ['West of USA ', 'US Center ', 'South of  USA ', 'East of USA', 'Canada', 'Caribbean']
n_america_regions = order_regions[order_regions.index.isin(n_america)]

def handle_region(value):
    if(value in n_america_regions):
        return 'North America'
    else:
        return value

df['Order Region'] = df['Order Region'].apply(handle_region)

df['Order Region'].value_counts()

df['Customer State'].value_counts(ascending = False)

# Dictionary mapping abbreviations to state names
state_mapping = {
    'PR': 'Puerto Rico',
    'CA': 'California',
    'NY': 'New York',
    'TX': 'Texas',
    'IL': 'Illinois',
    'FL': 'Florida',
    'OH': 'Ohio',
    'PA': 'Pennsylvania',
    'MI': 'Michigan',
    'NJ': 'New Jersey',
    'AZ': 'Arizona',
    'GA': 'Georgia',
    'MD': 'Maryland',
    'NC': 'North Carolina',
    'CO': 'Colorado',
    'VA': 'Virginia',
    'OR': 'Oregon',
    'MA': 'Massachusetts',
    'TN': 'Tennessee',
    'NV': 'Nevada',
    'MO': 'Missouri',
    'HI': 'Hawaii',
    'CT': 'Connecticut',
    'UT': 'Utah',
    'NM': 'New Mexico',
    'LA': 'Louisiana',
    'WA': 'Washington',
    'WI': 'Wisconsin',
    'MN': 'Minnesota',
    'SC': 'South Carolina',
    'IN': 'Indiana',
    'DC': 'District of Columbia',
    'KY': 'Kentucky',
    'KS': 'Kansas',
    'DE': 'Delaware',
    'RI': 'Rhode Island',
    'WV': 'West Virginia',
    'OK': 'Oklahoma',
    'ND': 'North Dakota',
    'ID': 'Idaho',
    'AR': 'Arkansas',
    'MT': 'Montana',
    'IA': 'Iowa',
    'AL': 'Alabama'
}

# Replace abbreviations with state names
df['Customer State'] = df['Customer State'].replace(state_mapping)

df['Customer State'].value_counts(ascending = False)

df.head(30)

df['Delivery Status'].value_counts(ascending = False)

df.info()



"""# **Feature Selection**"""

fs_df=df.copy()

fs_df=fs_df.drop(['Latitude','Longitude','Order Profit Per Order','Order Item Profit Ratio'],axis=1)

fs_df.head()

fs_df['Type Of Payments'].value_counts(ascending = False)

fs_df['Type Of Payments'] = fs_df['Type Of Payments'].replace('PAYMENT', 'CREDIT')

fs_df['Type Of Payments'] = fs_df['Type Of Payments'].replace('TRANSFER', 'INTERNET BANKING')

fs_df['Type Of Payments'] = fs_df['Type Of Payments'].replace('CASH', 'COD')

fs_df['Type Of Payments'].value_counts(ascending = False)

fs_df.info()

"""Data Preprocessing for date

-Convert date columns into a numerical format that your model can understand.
Extracting the year, month, day, or day of the week as separate features.
"""

# Convert the 'date_column' to a datetime data type
fs_df['order date (DateOrders)'] = pd.to_datetime(fs_df['order date (DateOrders)'])

# Extract day, month, and year into separate columns
fs_df['order day'] = fs_df['order date (DateOrders)'].dt.day
fs_df['order month'] = fs_df['order date (DateOrders)'].dt.month

# Convert the 'date_column' to a datetime data type
date_format = "%d/%m/%Y"
fs_df['shipping date (DateOrders)'] = pd.to_datetime(fs_df['shipping date (DateOrders)'],format=date_format)

# Extract day, month, and year into separate columns
fs_df['shipping day'] = fs_df['shipping date (DateOrders)'].dt.day
fs_df['shipping month'] = fs_df['shipping date (DateOrders)'].dt.month

fs_df=fs_df.drop(['shipping date (DateOrders)','order date (DateOrders)'],axis=1)

fs_df['Order Status'] = fs_df['Order Status'].replace('PENDING', 'PENDING_MANUFACTURING')

fs_df = fs_df[~fs_df['Order Status'].isin(['CANCELED', 'CLOSED'])]

fs_df['Order Status'] = fs_df['Order Status'].replace('COMPLETE', 'COMPLETE_PACKING')

fs_df['Type Of Payments'].value_counts(ascending = False)

fs_df['Order Status'].value_counts(ascending = False)

fs_df.info()

# Selects the categorical columns from the training dataset.
cat_columns = fs_df.select_dtypes(include='object').columns

from sklearn.preprocessing import LabelEncoder
import pandas as pd

# Create a dictionary to store encoded mappings
encoded_values = {}

# Loop through categorical columns and encode them
for col in cat_columns:
    le = LabelEncoder()
    fs_df[col] = le.fit_transform(fs_df[col])
    encoded_values[col] = dict(zip(le.classes_, le.transform(le.classes_)))


# Display the encoded mappings for each categorical column
for col, mapping in encoded_values.items():
    print(f'Encoded mapping for column "{col}":')
    print(mapping)
    print("")

"""### Feature Selection Using Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

X = fs_df.drop('Late_delivery_risk', axis=1)
y = fs_df['Late_delivery_risk']

model = RandomForestClassifier()
model.fit(X, y)

feature_importances = pd.Series(model.feature_importances_, index=X.columns)
relevant_features1 = feature_importances.sort_values(ascending=False)
print(relevant_features1)

"""### Feature Selection Using Chi-Squared"""

from sklearn.feature_selection import SelectKBest, chi2

X = fs_df.drop('Late_delivery_risk', axis=1)
y = fs_df['Late_delivery_risk']

selector = SelectKBest(score_func=chi2, k='all')
selector.fit(X, y)

relevant_features2 = pd.DataFrame({'Feature': X.columns, 'Score': selector.scores_})
relevant_features2 = relevant_features2.sort_values(by='Score', ascending=False)
print(relevant_features2)

"""# Machine learning models for Predicting Late Delivery"""

from sklearn import metrics

import matplotlib.colors as mcolors

ml_df=df.copy()

ml_df['Type Of Payments'].value_counts(ascending = False)

ml_df['Type Of Payments'] = ml_df['Type Of Payments'].replace('PAYMENT', 'CREDIT')

ml_df['Type Of Payments'] = ml_df['Type Of Payments'].replace('TRANSFER', 'INTERNET BANKING')

ml_df['Type Of Payments'] = ml_df['Type Of Payments'].replace('CASH', 'COD')

ml_df['Type Of Payments'].value_counts(ascending = False)

ml_df=ml_df.drop(['Department Id', 'Order Item Quantity', 'Department Name',
                  'Product Category Id', 'Product Card Id', 'Product Price', 'Market', 'Category Name',
                  'Sales', 'Customer Segment','Order Item Discount Rate','Order Item Discount',
                  'Sales per customer', 'Order Region','Customer State','Order Id', 'Order Item Id', 'Order Customer Id',
                  'Latitude','Longitude','Order Profit Per Order','Order Item Profit Ratio','Days for shipping (real)'],axis=1)

"""Data Preprocessing for date

-Convert date columns into a numerical format that your model can understand.
Extracting the year, month, day, or day of the week as separate features.
"""

# Convert the 'date_column' to a datetime data type
ml_df['order date (DateOrders)'] = pd.to_datetime(ml_df['order date (DateOrders)'])

# Extract day, month, and year into separate columns
ml_df['order day'] = ml_df['order date (DateOrders)'].dt.day
ml_df['order month'] = ml_df['order date (DateOrders)'].dt.month

# Convert the 'date_column' to a datetime data type
date_format = "%d/%m/%Y"
ml_df['shipping date (DateOrders)'] = pd.to_datetime(ml_df['shipping date (DateOrders)'],format=date_format)

# Extract day, month, and year into separate columns
ml_df['shipping day'] = ml_df['shipping date (DateOrders)'].dt.day
ml_df['shipping month'] = ml_df['shipping date (DateOrders)'].dt.month

ml_df=ml_df.drop(['shipping date (DateOrders)','order date (DateOrders)'],axis=1)

ml_df['Order Status'] = ml_df['Order Status'].replace('PENDING', 'PENDING_MANUFACTURING')

ml_df = ml_df[~ml_df['Order Status'].isin(['CANCELED', 'CLOSED', 'COMPLETE'])]

ml_df['Order Status'].value_counts(ascending = False)

ml_df['Delivery Status'].value_counts(ascending = False)

ml_df= ml_df[ml_df['Delivery Status'] != 'Shipping canceled']

# Selects the categorical columns from the training dataset.
cat_columns = ml_df.select_dtypes(include='object').columns

from sklearn.preprocessing import LabelEncoder

cat_columns

le=LabelEncoder()
def Labelencoder_feature(x):
    le=LabelEncoder()
    x=le.fit_transform(x)
    return x

for i in cat_columns:
    ml_df[i]  = le.fit_transform(ml_df[i])

ml_df.head()

"""Shipping cancellation is a process where a previously arranged shipment is halted or canceled, often by the sender or the shipping company. It doesn't inherently provide information about whether a product will be delivered late. So we would filter out the Shipping Canceled data"""

ml_df.info()

ml_df.info()

late_x=ml_df.drop(['Delivery Status','Late_delivery_risk'],axis=1)

late_x.info()

late_x.nunique()

late_y=ml_df['Late_delivery_risk']

"""The classification models used in this project are Logistic Regression, Linear Discriminant Analysis, Gaussian Naive Bayes, Support Vector Machines, and Random Forest classification to predict "Late Delivery" based on on  accuracy, recall and F1 score metrics.

**Metrics:**

TP: The model correctly predicts that "Late Delivery" will occur. This allows the company's supply chain to prepare for such an outcome early enough to mitigate the effects.

TN: The model correctly predicts that the "Late Delivery" will not occur.

FP: The model incorrectly predicts that the "Late Delivery" will occur.This signifies that reduction resources have been wasted.

FN: The model incorrectly predicts that the "Late Delivery" will not occur. This might be the most undesirable consequence since the "Late Delivery" prediction goal is not met.


The model with the highest score of F1 is selected in this report ;

*F1 Score = (2 * Precision * Recall) / (Precision + Recall)*

where:

Precision: Correct positive predictions relative to total positive predictions

Recall: Correct positive predictions relative to total actual positives
"""
num_features = late_x.shape[1]
print(f"Number of features expected by the model: {num_features}")

from sklearn.model_selection import train_test_split
xlatedelivery_train, xlatedelivery_test,ylatedelivery_train,ylatedelivery_test = train_test_split(late_x,late_y,test_size = 0.3, random_state = 5)

"""I used the standard scaler to standardize the data."""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
xlatedelivery_train=sc.fit_transform(xlatedelivery_train)
xlatedelivery_test=sc.transform(xlatedelivery_test)

"""The models are evaluated using accuracy, recall, F1 score metrics since the output is in binary classification format. The F1 score is the primary metric used to measure the performance of different models."""

from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, f1_score

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier  # Import DecisionTreeClassifier

# Create a DecisionTreeClassifier with specified hyperparameters
DecisionTreeClassifier_model = DecisionTreeClassifier(max_depth=None, random_state=0)

# Fit the model on the training data
DecisionTreeClassifier_model.fit(xlatedelivery_train, ylatedelivery_train)

# Predict the late delivery status using the trained model
DecisionTreeClassifier_ylatedelivery_pred = DecisionTreeClassifier_model.predict(xlatedelivery_test)

# Calculate evaluation metrics
dt_accuracy_latedelivery = accuracy_score(DecisionTreeClassifier_ylatedelivery_pred, ylatedelivery_test)
dt_recall_latedelivery = recall_score(DecisionTreeClassifier_ylatedelivery_pred, ylatedelivery_test)
dt_conf_latedelivery = confusion_matrix(ylatedelivery_test, DecisionTreeClassifier_ylatedelivery_pred)
dt_f1_latedelivery = f1_score(ylatedelivery_test, DecisionTreeClassifier_ylatedelivery_pred)

# Print the results
print('Model parameters used are:', DecisionTreeClassifier_model)
print('Accuracy of late delivery status is:', dt_accuracy_latedelivery * 100, '%')
print('Recall score of late delivery status is:', dt_recall_latedelivery * 100, '%')
print('F1 score of late delivery status is:', dt_f1_latedelivery * 100, '%')

import pickle

# Specify the file path where you want to save the model
model_filename = 'dl_ml_model.pkl'

# Use pickle.dump() to save the model to the specified file path
with open(model_filename, 'wb') as model_file:
    pickle.dump(DecisionTreeClassifier_model, model_file)

# Verify that the model has been saved
print(f"Model has been saved to {model_filename}")